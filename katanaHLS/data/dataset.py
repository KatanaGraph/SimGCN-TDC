import os
from collections import defaultdict
import pickle
import pandas as pd
import numpy as np

import torch
from torch_geometric.data import InMemoryDataset, Data

from ..utils.utils import *

class SIPGraph(InMemoryDataset):
    def __init__(self, root, 
                name, sim_metric, threshold, train_val, test, load_cache = True,
                transform=None, pre_transform=None):
        
        self.name = name
        self.train_val = train_val        
        self.test = test  
        self.load_cache = load_cache

        # Concatenate the train_val and the test data
        self.data = pd.concat([self.train_val, self.test])

        self.similarity_function_map = {'Tanimoto': TanimotoSimilarity,
                                    "Dice": DiceSimilarity,
                                    "Cosine": CosineSimilarity,
                                    "Sokal": SokalSimilarity,
                                    "Russel": RusselSimilarity,
                                    "RogotGoldberg": RogotGoldbergSimilarity,
                                    "AllBit": AllBitSimilarity,
                                    "Kulczynski": KulczynskiSimilarity,
                                    "McConnaughey": McConnaugheySimilarity,
                                    "Asymmetric": AsymmetricSimilarity,
                                    "BraunBlanquet": BraunBlanquetSimilarity
                                }             
        self.sim_metric = sim_metric
        self.sim_metric_fnc = self.similarity_function_map[sim_metric]
        self.thres = threshold

        super().__init__(root, transform, pre_transform)
        
        # Load the similarity graph data 
        path = os.path.join(self.process_dir, self.graph_file)
        self.sim_graph, _ = torch.load(path)
        
        # Load the smiles to node id map
        path = os.path.join(self.process_dir, self.smiles_map_file)
        with open(path, 'rb') as file:
            self.map_smiles_to_idx = pickle.load(file)


    @property
    def processed_dir(self) -> str:
        # the directory where the of the processed data is kept
        self.process_dir = os.path.join(self.root, self.name, 'processed')
        return self.process_dir

    @property
    def processed_file_names(self):
        # The list of processed file names. If all of these files are not present,
        # then the process method will be called.
        
        self.graph_file = self.name + '_' + self.sim_metric + '_' + str(self.thres) +'.pyg'
        self.mat_file = self.name + '_' + self.sim_metric + '.npy'        
        self.feat_file = self.name + '_feat.npy'
        self.smiles_map_file = self.name + '_dict.pkl'

        if not self.load_cache: 
            return ['forced_processing']
        
        return [self.graph_file, 
                self.smiles_map_file,
                self.mat_file, 
                self.feat_file,                 
                ]

    def process(self):
        
        # We construct a graph based on the similarity between all pair of smiles
        # strings in the dataset. For quick look up of the node index corresponding
        # to a smiles string, we maintain a dictionary of mapping the smiles strings 
        # to node indices in the PyG graph. 
        # In the datasets, there are duplicates. So we maintain a list of indices corresponding 
        # to each smiles strings. 
        
        smiles_train = self.train_val['Drug'].values
        smiles_test = self.test['Drug'].values
        
        print('Unique smiles string in the train-validation set: {}/ {}'.format(len(np.unique(smiles_train)), len(smiles_train)))            
        print('Unique smiles string in the test set: {}/ {}'.format(len(np.unique(smiles_test)), len(smiles_test)))            

        smiles = self.data['Drug'].values
        print('Unique smiles string in the entire dataset: {}/ {}'.format(len(np.unique(smiles)), len(smiles)))
        
        labels = self.data['Y'].values
                
        # Store the smiles and node index mapping in a dictionary for easy look up.
        # Would be used for labelling training and validation data
        # We need this map because of the multiple distinct training and validation splits
        # are generated by the tdc competition. 
         
        path = os.path.join(self.process_dir ,self.smiles_map_file)
        if not (os.path.exists(path)) or not self.load_cache:
            print("Constructing smiles to node index mapping..." )
            self.map_smiles_to_idx = defaultdict(list)
            for index, smile in enumerate(smiles):
                self.map_smiles_to_idx[smile].append(index)

            with open(path, 'wb') as file:
                pickle.dump(self.map_smiles_to_idx, file, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            print('Loading smiles to index maps from cache.')
            with open(path, 'rb') as file:
                self.map_smiles_to_idx = pickle.load(file)
        print("Done.")
                
        # We construct the similarity matrix for all pairs of molecules in the 
        # dataset. This step is currently exhaustive and takes long time.
        # However, this can be potentially parallelized extensively. 
        # The sim_score_mat is a n x n matrix where n is the total number of 
        # smiles string. 
        
        path = os.path.join(self.process_dir, self.mat_file)
        if not (os.path.exists(path)) or not self.load_cache:
            self.sim_score_mat = construct_sim_matrices (smiles, self.sim_metric_fnc)
            np.save(path, self.sim_score_mat)
        else:
            print('Loading similarity matrix from cache.')
            self.sim_score_mat = np.load(path)
        
        # We construct the static feature vectors for each of the molecules in 
        # the dataset. smiles_feat is a tensor of dimension n x c where n
        # is the number of the smiles string anc c is the dimension of the features.
        
        path = os.path.join(self.process_dir, self.feat_file)
        if not (os.path.exists(path)) or not self.load_cache:
            self.smile_feat = construct_features(smiles)
            np.save(path, self.smile_feat)
            self.smile_feat = torch.tensor(self.smile_feat, dtype=torch.float)
        else:
            print('Loading node features from cache.')
            self.smile_feat = np.load(path)
            self.smile_feat = torch.tensor(self.smile_feat, dtype=torch.float)

        # Construct the COO representation of the graph using threshold on the 
        # similarity matrix 
        path = os.path.join(self.process_dir, self.graph_file)
        if not (os.path.exists(path)) or not self.load_cache:            
            self.edge_index = construct_edge_list(self.sim_score_mat, self.thres)

            # Attach labels to each node        
            y = torch.tensor(labels, dtype = torch.float)
            y = torch.reshape(y,(-1,1))

            # Attach training, validation and test labels to each node        
            # validations labels are by default set to false for all the nodes
            n_nodes = len(smiles)
            train_mask = torch.zeros(n_nodes, dtype=torch.bool)                
            val_mask = torch.zeros(n_nodes, dtype=torch.bool)   
            test_mask = torch.zeros(n_nodes, dtype=torch.bool)

            train_mask[:len(self.train_val)] = True                
            test_mask[len(self.train_val):] = True

            # Construct the top level PyG similarity graph Data object        
            # data_top = Data(x=self.smile_feat, x_graph=self.smiles_graph_feat, edge_index=self.edge_index, y=y, 
                # train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)
            data_top = Data(x=self.smile_feat, edge_index=self.edge_index, y=y, 
                train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)

            data_top.n_id = torch.arange(data_top.num_nodes)
        
            data_top, slices = self.collate([data_top])
            torch.save((data_top, slices), path)

        
        
        